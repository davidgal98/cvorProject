{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import weight_norm\n",
    "import cv2\n",
    "import os\n",
    "import itertools\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There are 96 feature files in each fold but they seem identical - diffrent features for the same frame\n",
    "there are 100 label files from each kind, gesture, right tool and left tool, 300 total\n",
    "there are 100 videos from each angle, frame collections. top view and side view, 200 total\n",
    "there are 100 kinematics files and 100 kinematict numpy files\n",
    "\n",
    "\n",
    "There is a concern of using an architecture without pretraining, but the features we are supplied with might overcome that\n",
    "\n",
    "* its possible that the kinematics are not aligned\n",
    "need a clean data and data loader\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/datashare/APAS\"\n",
    "\n",
    "clipSize = 30\n",
    "BATCH_SIZE = 20\n",
    "UNDER_SAMPLE_RATE = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gestures = {\"no gesture\" : \"G0\",\n",
    "\"needle passing\" : \"G1\",\n",
    "\"pull the suture\": \"G2\",\n",
    "\"Instrument tie\": \"G3\",\n",
    "\"Lay the knot\" : \"G4\",\n",
    "\"Cut the suture\" :\"G5\"}\n",
    "\n",
    "tool_usage ={\"no tool in hand\" : \"T0\",\n",
    " \"needle_driver\": \"T1\",\n",
    " \"forceps\": \"T2\",\n",
    " \"scissors\":\"T3\"}\n",
    "\n",
    "with open(os.path.join(DATA_ROOT, \"mapping_gestures.txt\"), 'r') as f1, \\\n",
    "    open(os.path.join(DATA_ROOT, \"mapping_tools.txt\"), 'r') as f2:\n",
    "        gestures_mapping = {int(k): v for k, v in [x.split() for x in f1.readlines()]}\n",
    "        tools_mapping = {int(k): v for k, v in [x.split() for x in f2.readlines()]}\n",
    "\n",
    "gestures = dict(gestures, **{v: k for k, v in gestures.items()})\n",
    "tool_usage = dict(tool_usage, **{v: k for k, v in tool_usage.items()})\n",
    "gestures_mapping = dict(gestures_mapping, **{v: k for k, v in gestures_mapping.items()})\n",
    "tools_mapping = dict(tools_mapping, **{v: k for k, v in tools_mapping.items()})\n",
    "\"\"\"\n",
    "now the dictionaries are from the G# to either name or number , same for T#\n",
    "\"\"\"\n",
    "\n",
    "gestures_mapping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will load the list of videos that have features\n",
    "We load all the folds definitions and check all files listed exist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "usedVideos = {}\n",
    "for vid in os.listdir(os.path.join(DATA_ROOT, \"features\", \"fold0\")):\n",
    "    usedVideos[vid.split('.')[0]] = vid\n",
    "print(len(usedVideos))\n",
    "#usedVideos\n",
    "\n",
    "folds = {}\n",
    "for fold in os.listdir(os.path.join(DATA_ROOT,\"folds\")):\n",
    "    with open(os.path.join(DATA_ROOT,\"folds\", fold)) as f:\n",
    "        vids = [x.strip('.csv\\n') for x in f.readlines()]\n",
    "        for vid in vids:\n",
    "            if vid in usedVideos.keys():\n",
    "                continue\n",
    "            else:\n",
    "                print(vid)\n",
    "        folds[fold.split('.')[0]] = vids\n",
    "\n",
    "#folds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will load the gestures labels and check for validity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "badVideos = []\n",
    "goldGestures = {}\n",
    "for vid in os.listdir(os.path.join(DATA_ROOT,\"transcriptions_gestures\")):\n",
    "    with open(os.path.join(DATA_ROOT,\"transcriptions_gestures\", vid)) as f:\n",
    "        gesturesSeq = [x.split(\"\\n\")[0].split(\" \") for x in f.readlines()]\n",
    "        vidName = vid.split('.')[0]\n",
    "        if vidName not in usedVideos.keys():\n",
    "            print(vidName, \" should not be used, missing features\")\n",
    "            badVideos.append(vidName)\n",
    "        for i in range(0, len(gesturesSeq)):\n",
    "            start, end, ges = gesturesSeq[i]\n",
    "            if ges not in gestures:\n",
    "                print(vidName, gesturesSeq[i], \"not known gesture\")\n",
    "                badVideos.append(vidName)\n",
    "            if i == 0:\n",
    "                if int(start) != 0:\n",
    "                    print(vidName, gesturesSeq[i], \"doesnt start from 0\")\n",
    "                    badVideos.append(vidName)\n",
    "            if i != 0:\n",
    "                if int(end) <= int(start):\n",
    "                    print(vidName, gesturesSeq[i], \"bad order\")\n",
    "                    badVideos.append(vidName)\n",
    "                if int(start) != int(gesturesSeq[i-1][1]) + 1:\n",
    "                    print(vidName, gesturesSeq[i], \"part missing\")\n",
    "                    badVideos.append(vidName)\n",
    "        goldGestures[vidName] = gesturesSeq\n",
    "\n",
    "print(badVideos)\n",
    "#goldGestures"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "will do the same for tools"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "goldLeft = {}\n",
    "for vid in os.listdir(os.path.join(DATA_ROOT,\"transcriptions_tools_left_new\")):\n",
    "    with open(os.path.join(DATA_ROOT,\"transcriptions_tools_left_new\", vid)) as f:\n",
    "        leftSeq = [x.split(\"\\n\")[0].split(\" \") for x in f.readlines()]\n",
    "        vidName = vid.split('.')[0]\n",
    "        if vidName not in usedVideos.keys():\n",
    "            print(vidName, \" should not be used, missing features\")\n",
    "            badVideos.append(vidName)\n",
    "        for i in range(0, len(leftSeq)):\n",
    "            start, end, tool = leftSeq[i]\n",
    "            if tool not in tool_usage:\n",
    "                print(vidName, leftSeq[i], \"not known gesture\")\n",
    "                badVideos.append(vidName)\n",
    "            if i == 0:\n",
    "                if int(start) != 0:\n",
    "                    print(vidName, leftSeq[i], \"doesnt start from 0\")\n",
    "                    badVideos.append(vidName)\n",
    "            if i != 0:\n",
    "                if int(end) <= int(start):\n",
    "                    print(vidName, leftSeq[i], \"bad order\")\n",
    "                    badVideos.append(vidName)\n",
    "                if int(start) != int(leftSeq[i-1][1]) + 1:\n",
    "                    print(vidName, leftSeq[i], \"part missing\")\n",
    "                    badVideos.append(vidName)\n",
    "        goldLeft[vidName] = leftSeq\n",
    "print(badVideos)\n",
    "#goldLeft"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "goldRight = {}\n",
    "for vid in os.listdir(os.path.join(DATA_ROOT,\"transcriptions_tools_right_new\")):\n",
    "    with open(os.path.join(DATA_ROOT,\"transcriptions_tools_right_new\", vid)) as f:\n",
    "        rightSeq = [x.split(\"\\n\")[0].split(\" \") for x in f.readlines()]\n",
    "        vidName = vid.split('.')[0]\n",
    "        if vidName not in usedVideos.keys():\n",
    "            print(vidName, \" should not be used, missing features\")\n",
    "            badVideos.append(vidName)\n",
    "        for i in range(0, len(rightSeq)):\n",
    "            start, end, tool = rightSeq[i]\n",
    "            if tool not in tool_usage:\n",
    "                print(vidName, rightSeq[i], \"not known gesture\")\n",
    "                badVideos.append(vidName)\n",
    "            if i == 0:\n",
    "                if int(start) != 0:\n",
    "                    print(vidName, rightSeq[i], \"doesnt start from 0\")\n",
    "                    badVideos.append(vidName)\n",
    "            if i != 0:\n",
    "                if int(end) <= int(start):\n",
    "                    print(vidName, rightSeq[i], \"bad order\")\n",
    "                    badVideos.append(vidName)\n",
    "                if int(start) != int(rightSeq[i-1][1]) + 1:\n",
    "                    print(vidName, rightSeq[i], \"part missing\")\n",
    "                    badVideos.append(vidName)\n",
    "        goldRight[vidName] = rightSeq\n",
    "\n",
    "print(badVideos)\n",
    "#goldRight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we will check there  are  kinematics for each video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kinematicsFiles = {}\n",
    "for vid in os.listdir(os.path.join(DATA_ROOT,\"kinematics_npy\")):\n",
    "    vidName = vid.split('.')[0]\n",
    "    if vidName not in usedVideos.keys():\n",
    "        print(vidName)\n",
    "    kinematicsFiles[vidName] = vid\n",
    "print(len(kinematicsFiles))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will ignore the videos with broken labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "badVideos = set(badVideos)\n",
    "\n",
    "def delete_multiple_keys(dict, keysToDelete):\n",
    "    for key in keysToDelete:\n",
    "        if key in dict.keys():\n",
    "            del dict[key]\n",
    "\n",
    "delete_multiple_keys(usedVideos, badVideos)\n",
    "delete_multiple_keys(goldGestures, badVideos)\n",
    "delete_multiple_keys(goldLeft, badVideos)\n",
    "delete_multiple_keys(goldRight, badVideos)\n",
    "delete_multiple_keys(kinematicsFiles, badVideos)\n",
    "\n",
    "print([len(folds[fold]) for fold in folds.keys()])\n",
    "\n",
    "for fold in folds.keys():\n",
    "    for vid in folds[fold]:\n",
    "        if vid in badVideos:\n",
    "            folds[fold].remove(vid)\n",
    "\n",
    "print(len(usedVideos))\n",
    "print(len(goldGestures))\n",
    "print(len(goldLeft))\n",
    "print(len(goldRight))\n",
    "print(len(kinematicsFiles))\n",
    "print([len(folds[fold]) for fold in folds.keys()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will load the features and kinematics and check synchronization between them and the labels\n",
    "also we will transform the labels to vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = {}\n",
    "foldName = \"fold0\"\n",
    "foldFeatures = {}\n",
    "for vid in usedVideos.keys():\n",
    "    foldFeatures[vid] = np.load(os.path.join(DATA_ROOT, \"features\", foldName, usedVideos[vid]))\n",
    "features[foldName] = foldFeatures\n",
    "print(len(foldFeatures))\n",
    "\n",
    "kinematicFeatures = {}\n",
    "for vid in usedVideos.keys():\n",
    "    kinematicFeatures[vid] = np.load(os.path.join(DATA_ROOT, \"kinematics_npy\", usedVideos[vid]))\n",
    "print(len(kinematicFeatures))\n",
    "\n",
    "def segmentsToVector(segments):\n",
    "    vec = []\n",
    "    for start, end, label in segments:\n",
    "        for i in range(int(start),int(end) + 1):\n",
    "            vec.append(label)\n",
    "    if int(end)+1 != len(vec):\n",
    "        print(\"opps\", int(end), len(vec))\n",
    "    return vec\n",
    "\n",
    "goldGesturesVectors = {}\n",
    "goldRightVectors = {}\n",
    "goldLeftVectors = {}\n",
    "for vid in usedVideos.keys():\n",
    "    goldGesturesVectors[vid] = segmentsToVector(goldGestures[vid])\n",
    "    goldRightVectors[vid] = segmentsToVector(goldRight[vid])\n",
    "    goldLeftVectors[vid] = segmentsToVector(goldLeft[vid])\n",
    "\n",
    "#print(goldGesturesVectors[vid])\n",
    "#print(goldRightVectors[vid])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vid_features_gest_right_left_kinematics = [[x, np.shape(y), int(goldGestures[x][-1][1]), int(goldRight[x][-1][1]), int(goldLeft[x][-1][1]), np.shape(kinematicFeatures[x])] for x,y in foldFeatures.items()]\n",
    "[[x, x[1][1] == x[2] and x[2] == x[3] and x[3] == x[4] and x[4] == x[5][1], x[1][1] == x[2] and x[2] == x[3] and x[3] == x[4], max(x[1][1],x[2],x[3],x[4],x[5][1]) - min(x[1][1],x[2],x[3],x[4],x[5][1])] for x in vid_features_gest_right_left_kinematics]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "as we can see the data is still problematic, there is no good alignment between the features, labels and kinematics\n",
    "we will take the shortest length for each video and cut the data accordingly\n",
    "We will under sample the data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shortestLengths = [(x[0] , min(x[1][1],x[2],x[3],x[4],x[5][1])) for x in vid_features_gest_right_left_kinematics]\n",
    "\n",
    "def underSample(array):\n",
    "    return [array[j] for j in range(0,len(array),UNDER_SAMPLE_RATE)]\n",
    "\n",
    "#shortestLengths\n",
    "for i, (vid, shortestLength) in enumerate(shortestLengths):\n",
    "    goldGesturesVectors[vid] = underSample(goldGesturesVectors[vid][0:shortestLength])\n",
    "    goldRightVectors[vid] = underSample(goldRightVectors[vid][0:shortestLength])\n",
    "    goldLeftVectors[vid] = underSample(goldLeftVectors[vid][0:shortestLength])\n",
    "    kinematicFeatures[vid] = underSample(np.transpose(kinematicFeatures[vid])[0:shortestLength])\n",
    "    foldFeatures[vid] = underSample(np.transpose(foldFeatures[vid])[0:shortestLength])\n",
    "    shortestLengths[i] = (vid, int(shortestLength/UNDER_SAMPLE_RATE) + 1)\n",
    "\n",
    "#print(np.shape(goldGesturesVectors[vid]))\n",
    "#print(np.shape(goldRightVectors[vid]))\n",
    "#print(np.shape(goldLeftVectors[vid]))\n",
    "#print(np.shape(kinematicFeatures[vid]))\n",
    "#print(np.shape(foldFeatures[vid]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shortestLengths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature Engineering\n",
    "we will calculate the movement of each hand and the distance between the hands and the movement between that distances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# There are 15 measurements per sensor, there are 6 sensors, 3 for each hand\n",
    "# we will guess the first is left hand the second is the right hand - confirmed by looking at the data, right hand moves and sensors 4,5,6 change position\n",
    "NUMBER_OF_MEASUREMENTS = 6 # should be 15\n",
    "NUMBER_OF_INTERESTING_MEASUREMENTS = 6 # 3 pos and 3 angles\n",
    "LOCATION_AXIS = 3\n",
    "NUMBER_OF_SENSORS_PER_HAND = 3\n",
    "\n",
    "rightKinematicsFeatures = {}\n",
    "leftKinematicsFeatures = {}\n",
    "\n",
    "def flattArray(array):\n",
    "    tmp = array[0]\n",
    "    for i in range(1, len(array)):\n",
    "        tmp = np.concatenate((tmp, array[i]), axis=None)\n",
    "    return np.reshape(tmp,(1, len(tmp)))\n",
    "\n",
    "for vid in usedVideos.keys():\n",
    "    print(vid)\n",
    "    rightKinematicsFeatures[vid] = []\n",
    "    leftKinematicsFeatures[vid] = []\n",
    "    #print(np.shape(kinematicFeatures[vid]))\n",
    "    for i in range(0, len(kinematicFeatures[vid])):\n",
    "        samples = [np.array(kinematicFeatures[vid][i][x*NUMBER_OF_MEASUREMENTS:(x*NUMBER_OF_MEASUREMENTS) + NUMBER_OF_INTERESTING_MEASUREMENTS]) for x in range(0,NUMBER_OF_SENSORS_PER_HAND*2)]\n",
    "        left = samples[0:NUMBER_OF_SENSORS_PER_HAND]\n",
    "        right = samples[NUMBER_OF_SENSORS_PER_HAND:NUMBER_OF_SENSORS_PER_HAND*2]\n",
    "        # adding difference between the sensor on each hand:\n",
    "        # adding distance as well\n",
    "        left.append(np.linalg.norm(left[0][0:LOCATION_AXIS] - left[-1][0:LOCATION_AXIS]))\n",
    "        right.append(np.linalg.norm(right[0][0:LOCATION_AXIS] - right[-1][0:LOCATION_AXIS]))\n",
    "        for j in range(1,NUMBER_OF_SENSORS_PER_HAND):\n",
    "            left.append(left[j] - left[j-1])\n",
    "            left.append(np.linalg.norm(left[j][0:LOCATION_AXIS] - left[j-1][0:LOCATION_AXIS]))\n",
    "            right.append(right[j] - right[j-1])\n",
    "            right.append(np.linalg.norm(right[j][0:LOCATION_AXIS] - right[j-1][0:LOCATION_AXIS]))\n",
    "        # adding difference between the hands:\n",
    "        # adding distance as well\n",
    "        for j in range(0,NUMBER_OF_SENSORS_PER_HAND):\n",
    "            diff = left[j] - right[j]\n",
    "            left.append(diff)\n",
    "            right.append(diff)\n",
    "            dist= np.linalg.norm(diff[0:LOCATION_AXIS])\n",
    "            left.append(dist)\n",
    "            right.append(dist)\n",
    "        # converting to numpy array\n",
    "        left = flattArray(left)\n",
    "        right = flattArray(right)\n",
    "        # adding speeds - diffrence from last sample\n",
    "        if i == 0:\n",
    "            left = np.concatenate((left,np.zeros((np.shape(left)))), axis=1)\n",
    "            right = np.concatenate((right,np.zeros((np.shape(right)))), axis=1)\n",
    "        else:\n",
    "            left = np.concatenate((left,left - leftKinematicsFeatures[vid][i-1][0][0:np.size(left)]), axis=1)\n",
    "            right = np.concatenate((right,right - rightKinematicsFeatures[vid][i-1][0][0:np.size(right)]), axis=1)\n",
    "        leftKinematicsFeatures[vid].append(left)\n",
    "        rightKinematicsFeatures[vid].append(right)\n",
    "        # Experiment\n",
    "        # option 1: adding 1 0 to left and 0 1 to right for training a single network for both hands\n",
    "        # option 2: training two networks one for right and one for left hand\n",
    "        #print(np.shape(samples))\n",
    "        #print(vid,samples)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.shape(rightKinematicsFeatures[vid])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a usable data set, we will transform it to a data set of clips containing 30 frames and a single label for the clip\n",
    "We will create a data set class , the class need to implement __getitem__(self, index)\n",
    "meaning we can hold the data however we want but need to return a sample by index\n",
    "a List of tuples of np.array is a good idea"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TheDataSet = {}\n",
    "for vid, shortestLength in shortestLengths:\n",
    "    print(np.shape(leftKinematicsFeatures[vid]))\n",
    "    print(np.shape(rightKinematicsFeatures[vid]))\n",
    "    TheDataSet[vid] = []\n",
    "    for i in range(0, shortestLength, clipSize):\n",
    "        if i + clipSize < shortestLength:\n",
    "            clip = []\n",
    "            clip.append(goldGesturesVectors[vid][i:i+clipSize])\n",
    "            clip.append(goldRightVectors[vid][i:i+clipSize])\n",
    "            clip.append(goldLeftVectors[vid][i:i+clipSize])\n",
    "            clip.append(leftKinematicsFeatures[vid][i:i+clipSize])\n",
    "            clip.append(rightKinematicsFeatures[vid][i:i+clipSize])\n",
    "            clip.append(foldFeatures[vid][i:i+clipSize])\n",
    "            TheDataSet[vid].append(clip)\n",
    "    #print(len(TheDataSet[vid]))\n",
    "\n",
    "trainData = {}\n",
    "validData = {}\n",
    "testData = {}\n",
    "foldNumber = 0\n",
    "for vid in usedVideos.keys():\n",
    "    if vid in folds[\"test \" + str(foldNumber)]:\n",
    "        testData[vid] = TheDataSet[vid]\n",
    "    elif vid in folds[\"valid \" + str(foldNumber)]:\n",
    "        validData[vid] = TheDataSet[vid]\n",
    "    else:\n",
    "        trainData[vid] = TheDataSet[vid]\n",
    "print(len(trainData))\n",
    "print(len(validData))\n",
    "print(len(testData))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class baseDataset(Dataset):\n",
    "    def __init__(self, targets, clip_size=30):\n",
    "        self.targets = targets\n",
    "        self.clip_size = clip_size\n",
    "        self.y_gesture = 0\n",
    "        self.y_right = 1\n",
    "        self.y_left = 2\n",
    "        self.leftKinematics = 3\n",
    "        self.rightKinematics = 4\n",
    "        self.features = 5\n",
    "        self.data = []\n",
    "\n",
    "        for vid in targets.keys():\n",
    "            for clip in targets[vid]:\n",
    "                self.data.append(clip)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        clip = self.data[index]\n",
    "        return (np.array(self.convertLabels(clip[self.y_gesture], gestures_mapping)),\n",
    "                         np.array(self.convertLabels(clip[self.y_right], tools_mapping)),\n",
    "                         np.array(self.convertLabels(clip[self.y_left], tools_mapping)),\n",
    "                         np.array(clip[self.leftKinematics]),\n",
    "                         np.array(clip[self.rightKinematics]),\n",
    "                         np.array(clip[self.features]))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def convertLabels(self, vec, map):\n",
    "        counts = {}\n",
    "        maxValue = 0\n",
    "        maxLabel = None\n",
    "        for i in vec:\n",
    "            if i in counts:\n",
    "                counts[i] += 1\n",
    "            else:\n",
    "                counts[i] = 1\n",
    "        for k,v in counts.items():\n",
    "            if v > maxValue:\n",
    "                maxValue = v\n",
    "                maxLabel = k\n",
    "        return map[maxLabel]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainDataset = baseDataset(trainData, clipSize)\n",
    "validDataset = baseDataset(validData, clipSize)\n",
    "testDataset = baseDataset(testData, clipSize)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "t = trainDataset.__getitem__(20)\n",
    "for i in t:\n",
    "    print(np.shape(i))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Working with the gpu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validLoader = DataLoader(validDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testLoader = DataLoader(testDataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(108 * clipSize, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 4)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "leftTinymodel = TinyModel()\n",
    "rightTinymodel = TinyModel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs,  kernel_size,stride=stride, dilation=dilation, padding=1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,stride=stride, dilation=dilation, padding=1))\n",
    "        self.net = nn.Sequential(self.conv1, self.relu, self.dropout, self.conv2, self.relu, self.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class SingleMaTcnKinModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleMaTcnKinModel, self).__init__()\n",
    "        self.tcn = TemporalConvNet( num_inputs=108, num_channels=[128,64,32, 64], kernel_size=2, dropout=0.2)\n",
    "        self.linear1 = torch.nn.Linear(1024, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 4)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        self.fullnet = nn.Sequential(self.linear1, self.activation, self.linear2, self.softmax)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tcn(x)\n",
    "        return self.fullnet(torch.reshape(x, (np.shape(x)[0], 1024)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "leftModel = SingleMaTcnKinModel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(leftTinymodel.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(trainLoader):\n",
    "        # Every data instance is an input + label pair\n",
    "        ges, right, left, leftkin, rightkin, fea = data\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = leftTinymodel(torch.reshape(leftkin, (np.shape(leftkin)[0], 108 * clipSize)).to(torch.float32))\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, left.to(torch.long))\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(trainLoader) + i + 1\n",
    "            print('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    leftTinymodel.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    leftTinymodel.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validLoader):\n",
    "        vges, vright, vleft, vleftkin, vrightkin, vfea = vdata\n",
    "        voutputs = leftTinymodel(torch.reshape(vleftkin, (np.shape(vleftkin)[0], 108 * clipSize)).to(torch.float32))\n",
    "        vloss = loss_fn(voutputs, vleft.to(torch.long))\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    print('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(leftTinymodel.state_dict(), os.path.join(\"models\", model_path))\n",
    "\n",
    "    epoch_number += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We would like to analyze  the performance of our models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "saved_model = tinymodel()\n",
    "saved_model.load_state_dict(torch.load(PATH))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compareToolUsage(predictions, groundTruth):\n",
    "    print(\"segmentation performance\")\n",
    "    precision = metrics.precision_score(groundTruth, predictions, average=\"weighted\")\n",
    "    print(\"weighted precision: \", precision)\n",
    "    recall = metrics.recall_score(groundTruth, predictions, average=\"weighted\")\n",
    "    print(\"weighted recall: \", recall)\n",
    "    f1_micro = metrics.f1_score(groundTruth, predictions, average=\"micro\")\n",
    "    print(\"f1_micro: \", f1_micro)\n",
    "    f1_macro = metrics.f1_score(groundTruth, predictions, average=\"macro\")\n",
    "    print(\"f1_macro: \", f1_macro)\n",
    "    accuracy = metrics.accuracy_score(groundTruth, predictions)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    return [precision, recall, f1_micro, f1_macro, accuracy]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
